{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XC4xwctYhsnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install libtesseract-dev libleptonica-dev"
      ],
      "metadata": {
        "id": "qRdbb6Padfuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tesserocr"
      ],
      "metadata": {
        "id": "oLf7oANGpKAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six PyPDF2 pymupdf wrapt_timeout_decorator"
      ],
      "metadata": {
        "id": "3ROu6jpidGKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parse PDF-files"
      ],
      "metadata": {
        "id": "qgC_XrDDbwka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from io import StringIO\n",
        "import re\n",
        "import os\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "import fitz\n",
        "from wrapt_timeout_decorator import timeout\n",
        "from logging import getLogger\n",
        "from tqdm import tqdm\n",
        "\n",
        "_threshold_intersection = 0.3  # if the intersection is large enough.\n",
        "\n",
        "logger = getLogger()\n",
        "def normalize_spaces(text: str) -> str:\n",
        "    \"\"\"Removes single line breaks to highlight paragraphs correctly\"\"\"\n",
        "    return re.sub(\"[ ]{1,}\", \" \",\n",
        "                  re.sub(\"(?<!\\n)\\n(?!\\n){1,}\", \" \",\n",
        "                         text)).strip()\n",
        "\n",
        "\n",
        "def _check_contain(r_word, points):\n",
        "    \"\"\"If `r_word` is contained in the rectangular area.\n",
        "\n",
        "    The area of the intersection should be large enough compared to the\n",
        "    area of the given word.\n",
        "\n",
        "    Args:\n",
        "        r_word (fitz.Rect): rectangular area of a single word.\n",
        "        points (list): list of points in the rectangular area of the\n",
        "            given part of a highlight.\n",
        "\n",
        "    Returns:\n",
        "        bool: whether `r_word` is contained in the rectangular area.\n",
        "    \"\"\"\n",
        "    r = fitz.Quad(points).rect\n",
        "    r.intersect(r_word)\n",
        "\n",
        "    if r.get_area() >= r_word.get_area() * _threshold_intersection:\n",
        "        contain = True\n",
        "    else:\n",
        "        contain = False\n",
        "    return contain\n",
        "\n",
        "\n",
        "def _extract_annot(annot, words_on_page):\n",
        "    \"\"\"Extract words in a given highlight.\n",
        "\n",
        "    Args:\n",
        "        annot (fitz.Annot): [description]\n",
        "        words_on_page (list): [description]\n",
        "\n",
        "    Returns:\n",
        "        str: words in the entire highlight.\n",
        "    \"\"\"\n",
        "\n",
        "    quad_points = annot.vertices\n",
        "    quad_count = int(len(quad_points) / 4)\n",
        "    sentences = ['' for i in range(quad_count)]\n",
        "    for i in range(quad_count):\n",
        "        points = quad_points[i * 4: i * 4 + 4]\n",
        "        words = [\n",
        "            w for w in words_on_page if\n",
        "            _check_contain(fitz.Rect(w[:4]), points)\n",
        "        ]\n",
        "        sentences[i] = ' '.join(w[4] for w in words)\n",
        "    sentence = ' '.join(sentences)\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\n",
        "@timeout(5, use_signals=False)\n",
        "def extract_annots(path: str):\n",
        "    \"\"\"For annotation extraction from pdf files\"\"\"\n",
        "    doc = fitz.open(path)\n",
        "\n",
        "    highlights_d = {}\n",
        "    for page_num, page in enumerate(doc):\n",
        "        try:\n",
        "            wordlist = page.get_text(\"words\")\n",
        "            highlights = []\n",
        "            annot = page.first_annot\n",
        "            while annot:\n",
        "                phrase = _extract_annot(annot, wordlist)\n",
        "                highlights.append(phrase)\n",
        "                annot = annot.next\n",
        "\n",
        "                highlights_d[page_num] = highlights\n",
        "        except Exception as ex:\n",
        "            logger.info(f'Error in extract_annots {ex}')\n",
        "    return highlights_d\n",
        "\n",
        "@timeout(2, use_signals=False)\n",
        "def read_pdf_page(page, codec: str):\n",
        "    \"\"\"Read pdf page\"\"\"\n",
        "    rsrcmgr = PDFResourceManager()\n",
        "    retstr = StringIO()\n",
        "    laparams = LAParams()\n",
        "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
        "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "    interpreter.process_page(page)\n",
        "    return retstr.getvalue()\n",
        "\n",
        "\n",
        "@timeout(120, use_signals=False)\n",
        "def convert_pdf_to_txt(path: str, codec: str = \"utf-8\"):\n",
        "    with open(path, \"rb\") as fp:\n",
        "        pages = PDFPage.get_pages(fp,\n",
        "                                  set(),\n",
        "                                  maxpages=0,\n",
        "                                  password=\"\",\n",
        "                                  caching=True,\n",
        "                                  check_extractable=True)\n",
        "        parsed_result = {}\n",
        "\n",
        "\n",
        "        try:\n",
        "            annots = extract_annots(path)\n",
        "        except Exception as ex:\n",
        "            logger.error(f'Cant get annotations from  {path} because {str(ex)}')\n",
        "            annots = {}\n",
        "        for page_num, page in enumerate(pages):\n",
        "            try:\n",
        "              text = normalize_spaces(read_pdf_page(page, codec))\n",
        "\n",
        "              parsed_result[page_num] = {\"text\": text,\n",
        "                                        \"annots\": annots.get(page_num, [])}\n",
        "              if text.startswith(\"Графическая часть\"):\n",
        "                  # пропускаем картинки\n",
        "                  logger.warning(f'Skip image page {page_num} from {path}')\n",
        "                  break\n",
        "            except Exception as ex:\n",
        "                # пропускаем кривые страницы с изображениями\n",
        "                logger.error(f'Skip image page {page_num} from {path} becouse {str(ex)}')\n",
        "                parsed_result[page_num] = {\"text\": '',\n",
        "                                          \"annots\": []}\n",
        "\n",
        "    return parsed_result\n",
        "\n",
        "def parse_pdf_bucket(folder: str):\n",
        "    documents = {}\n",
        "\n",
        "    group_name = folder.split(\"/\")[-1]\n",
        "    documents[group_name] = documents.get(group_name, {})\n",
        "    for fn in tqdm(os.listdir(folder)):\n",
        "        print('FILE:', fn)\n",
        "        filename = os.path.join(folder, fn)\n",
        "        try:\n",
        "            res = convert_pdf_to_txt(filename)\n",
        "            documents[group_name][fn] = res\n",
        "        except Exception as ex:\n",
        "            logger.error(f'Error while document {filename} processing {str(ex)}')\n",
        "            documents[group_name][fn] = 'long time parsing'\n",
        "    return documents"
      ],
      "metadata": {
        "id": "uc3m7EICQpYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "folders = ['drive/MyDrive/jetfork_2023_dataset/doc1',\n",
        "           'drive/MyDrive/jetfork_2023_dataset/doc2',\n",
        "           'drive/MyDrive/jetfork_2023_dataset/ПД для ИИ'\n",
        "           ]\n",
        "\n",
        "all_documents = {}\n",
        "for folder in folders:\n",
        "  documents = parse_pdf_bucket(folder)\n",
        "  all_documents.update(documents)"
      ],
      "metadata": {
        "id": "Oxos_Hh5BRGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_documents.keys()"
      ],
      "metadata": {
        "id": "QTirEXeLCfjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib"
      ],
      "metadata": {
        "id": "Lnupw6sVniS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(all_documents, 'all_documents_v3.pkl')"
      ],
      "metadata": {
        "id": "zYX_1XjsMs5I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}